\section{Virtual memory}
\index{virtual memory}

In the good old days of the 8-bit microcomputers programs ran one at a time and
each one could access any part of the memory. Memory ranges were subdivided into
ROM (Read-Only Memory), RAM (Random-Access Memory), video memory, device memory
and various system-defined ranges (for example the
\href{https://worldofspectrum.org/ZXSpectrum128+3Manual/chapter8pt24.html}{Spectrum
  128} \autocite{spectrum128-manual:c8pt24}) and each program simply had to know
where they could and could not write to.\\

This posed problems -- even if you run one program at a time a bug might result in
you overwriting critical system state causing unexpected behaviour and most
likely a crash, necessitating a system restart. If you run more than one program
at once then you are in real trouble -- each program will need to somehow be
able to determine what parts of memory it can and cannot access, while
simultaneously being able to trample all over the data of both the system and
any other program. A single bug and you kill the system.\\

What is termed \index{fragmentation}\emph{fragmentation} -- the breaking up on
memory into smaller fragments -- is also a huge problem in this scenario as
every block of memory allocated by the operating system must sit in the `gaps'
left by all programs. If a program asks for more memory than a gap provides the
request must be refused.\\

As memory is not mediated by the operating system but accessed directly by
programs a solution to this issue must necessarily be implemented in
hardware. The solution is \emph{virtual memory} -- a mechanism where the
operating system is able to instruct a \emph{Memory Management Unit (MMU)} to
map `virtual' addresses to `physical' ones (we'll discuss how this is actually
implemented below).\\

Another major benefit of a virtual memory system is the ability to store
portions of programs on disk, i.e. \emph{paging}\index{paging}. This allows for
the ability to run programs larger in size than available memory as well as
allowing the system to free up memory to cache disk contents for quicker
access.\\

A \emph{physical address} \index{physical address} tells the CPU where a
particular byte resides -- whether it is in RAM (a particular DIMM, bank, row,
and column), a portion of memory assigned to system configuration
(e.g. \href{https://en.wikipedia.org/wiki/Advanced_Configuration_and_Power_Interface}{ACPI}
or
\href{https://en.wikipedia.org/wiki/Advanced_Programmable_Interrupt_Controller}{APIC}),
memory-mapped device registers (e.g. the
\href{https://en.wikipedia.org/wiki/PCI_configuration_space}{PCI configuration
  space} on a PC), or some other architecture-specific location.\\

From the kernel's point of view a physical address is simply an unsigned integer
(typically the same size as a
\href{https://en.wikipedia.org/wiki/Word_(computer_architecture)}{system
  word}). In Linux this is represented as a
\href{https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/include/linux/types.h?h=v5.17-rc7#n153}{phys\_addr\_t}.\\

As with much else in memory management a physical address is an abstraction over
far more complicated machinery, however from a kernel developer's point of view
we can simply view it as a pointer to a physical location.\\

Virtual addresses \index{virtual address} are also unsigned integers of the same
size as physical addresses, however they are (largely) arbitrary values which
can be mapped to any (page-aligned, see page tables section below) physical
address by the kernel.\\

An operating system can in theory implement virtual address mappings as it sees
fit -- the hardware allows for virtual mappings to be changed on demand -- however
Linux provides per-\emph{process} virtual mappings which are swapped out as the
scheduler switches processes (but not threads which share memory mappings).\\

Crucially the kernel is able to apply `flags' to virtual memory mappings which
control attributes such as marking it read-only, non-executable and much
else. This allows the kernel to restrict how memory is used and implement
features such as Copy On Write (covered in a later section in the book). Virtual
addresses can also be mapped to the same physical memory, a key mechanism for
allowing sharing of memory of memory-mapped files, dynamically-linked libraries
as well as clever inter-process communication (IPC) mechanisms.\\

When a process tries to access memory that is not mapped, MMU-capable hardware
raises a \index{page fault} page fault processor exception. This can be caught
by the operating system which can take any action it desires ranging from
providing a mapping for the virtual address attempting to be accessed to
segfaulting the process.\\

This is incredibly powerful as it allows for on-demand behaviour, principally
\href{https://en.wikipedia.org/wiki/Demand_paging}{demand paging} -- this is
where a memory range is marked internally by the kernel as valid even though no
physical memory is allocated yet.When the memory is \emph{demanded} then the
kernel page fault handling logic can perform either the physical allocation for
`anonymous memory' \index{anonymous memory} (i.e. memory in RAM not mapped to
any file on disk), or if the virtual address refers to a memory-mapped file it
can retrieve that from the disk, or if the memory has been paged out to disk
then it can be retrieved, as well as many other mechanisms entirely customisable
by the kernel.\\

Combining customisable page fault handing with virtual memory flagging allows
for clever mechanisms such as \index{Copy On Write} Copy On Write (COW) and
\index{zero page} zero pages -- the former is a key part of Linux's ability to
spin up new processes incredibly fast -- \index{fork}
\href{https://en.wikipedia.org/wiki/Fork_(system_call)}{forking} a process need
only set up new read-only page table mappings which can be copied only when a
user tries to write to them. Zero pages (a single zeroed physical page, see
below for a description of memory pages) can be mapped in to a faulting virtual
address being \emph{read} from without needing to use any additional memory at
all.\\

\subsection{Advantages of virtual memory}

Summarising the advantages of virtual memory:-

\begin{itemize}
  \item \textbf{Memory protection} -- Processes cannot access memory they are
    not permitted to such as unshared memory of other processes or kernel
    memory. If it were not for this a single process could bring down an entire
    system or both snoop on and modify sensitive data owned by another process
    or the kernel.
  \item \textbf{Virtual contiguity} -- We are able to use physical memory (at a
    page granularity, see page table section below) regardless of its
    fragmentation (i.e. how broken into parts physical memory is) therefore
    permitting efficient and simple memory allocation -- a process receiving a 1
    GiB block of memory doesn't need know or care that it's made from 1 part or
    10,000. Without this a process allocating several small blocks of memory
    could prevent any other from allocating a large one ever again.
  \item \textbf{Movability} -- While typically a process cares only about
    \emph{virtual} contiguity some, usually device-specific and in-kernel
    allocations, care about physical contiguity too. This can result in
    problematic fragmentation and thus more \emph{memory pressure} (high memory
    demand and less memory to service it) however this pressure can be relieved
    by moving physical memory around -- the mappings are virtual so can simply
    be updated.
  \item \textbf{Demand paging} -- As discussed above, with virtual mappings we
    are able to defer actually `backing' (i.e. physically allocating, paging in
    or mapping to a file depending on the mapping type) to the point where the
    memory is actually used. This allows for far more efficient use of memory,
    auto-expanding stack memory, paging memory to disk, fast forking and much
    else.
  \item \textbf{Memory attributes} -- By using virtual memory we are able to
    apply `flags` to blocks of memory and mark them as read-only, non-executable
    and much else. This allows for increased security and process stability -- a
    process cannot do things it should not do even with its own memory.
  \item \textbf{Shared memory} -- By having virtual memory mappings we are able
    to map different virtual addresses to the same underlying physical
    address. This allows for efficient dynamic linking of shared libraries,
    cached files to be loaded once into memory and referenced by separate
    processes without duplication (see the page cache section for more details)
    and clever Inter-Process Communication (IPC) mechanisms between processes.
\end{itemize}

\subsection{Page tables}
\index{page tables} Keeping a track of memory raises a practical issue -- if we
need metadata to keep track of things such as whether a certain part of memory
is allocated or not, then we can't keep track of things at a byte granularity or
we'd end up in the absurd situation where each byte of memory requires (at
least) a byte to describe it.\\

To work around this we divide memory up into finite chunks of a given size we
term \emph{pages}\index{pages}. The \emph{default} page size for x86-64 (and
many other architectures) is 4 KiB, or $2^{12}$ bytes.\\

Doing this significantly reduces the overhead required for virtual memory
mapping metadata, but by nowhere near enough. Consider a 32-bit address space --
this encompasses $2^{32} = 4,294,967,296$ bytes or $1,048,576$
pages. An array containing a 32-bit (4 byte) mapping for each of these would
require 4 MiB of space for page mappings for each process. 100 processes would
therefore consume 10\% of available memory just on virtual memory mappings.\\

For a 64-bit system the numbers become rather more prohibitive -- a 64-bit
system has $2^{64} = 18,446,744,073,709,551,616$ bytes of address space or
$4,503,599,627,370,496$ pages. An array containing a 64-bit (8 byte) mapping for
each of these would require 32 petabytes of memory for each process. A typical
modern-day 64-bit system only has access to 48 bits of physical memory which
reduces this figure to 512 GiB and considering user-accessible address space is
typically only half of that available (leaving the other half to the kernel) we
are left with a svelte 256 GiB per-process, and most of those null mappings.\\

Obviously it is therefore not practical to simply keep a simple array mapping
virtual addresses to physical ones. What is required is a \emph{sparse} series
of mappings which exclude most null mappings (i.e. unmapped memory). You could
conceive of many different schemes for subdividing these mappings -- a binary
tree representation for example perhaps combined with a bloom filter. However
these kind of techniques are complicated and have slow worst-case
performance. Since they are implemented in hardware and need to be as fast as
possible a simplistic mechanism is required.\\

The approach implemented in hardware is to use \emph{page tables} -- we
subdivide each virtual address into a series of offsets each (except the last)
indexing an entry in a \emph{page table} which is a single page (of the default
page size) containing the physical addresses of the next \emph{level} page table
for each valid offset. The last page table contains the physical addresses of
the actual \emph{data pages} containing the data referenced by each virtual
address with matching second-to-last offset, and the last offset contained in
the virtual address indexes the precise byte in the data page that the virtual
address references.\\

The preceding paragraph is somewhat difficult to unpack on first reading so
let's dig into this a little:-\\

Rather than mapping every single possible virtual address to a physical address,
we start with a top-level 'Page Global Directory' (PGD) page (of default size,
i.e. 4 KiB for x86-64). This is simply a page-sized array of physical
addresses. Since physical addresses are 8 bytes in a 64-bit system we are left
with $\frac{4096}{8} = 512 = 2^9$ entries. So if we take the top 9 bits of a
57-bit virtual address (more on why 57 bits shortly) and use it to offset into
this table, we either note that the entry is marked empty and thus the address
is invalid, or get the physical address to index into using the \emph{next 9 bit
offset} contained in the virtual address.\\

Recent high-end x86-64 hardware is capable of addressing 57 bits (128
petabytes!) of physical memory, despite having addresses 64 bits in length (most
consumer hardware at the time of writing supports 48 bits). Since we need the
final 12 bits to determine the referenced byte in the data page itself ($4096 =
2^{12}$) Therefore this arrangement requires $\frac{57 - 12}{9} = 5$ page table
levels.\\

Linux needs to be flexible enough to support hardware with differing addressable
physical memory and thus different numbers of page table levels. Since
\href{https://en.wikipedia.org/wiki/Intel_5-level_paging}{5-level paging} is the
highest requirement of supported hardware the abstractions within the kernel
have to assume 5 levels and then use clever tricks to collapse this into a
smaller number of levels should the hardware support less.\\

As a result kernel data structures specifically reference 5 levels:-

\begin{itemize}
\item Page Global Directory (PGD)
\item Page 4th Directory (P4D)
\item Page Upper Directory (PUD)
\item Page Middle Directory (PMD)
\item Page Table Entry directory (PTE)
\end{itemize}

Examining a 57-bit virtual address:-

\begin{center}
  $0xddd7314457cdea$ \\
  $011011101 110101110 011000101 000100010 101111100 110111101010$ \\
  \begin{tabular}{ c c c c c c }
    $011011101$ & $110101110$ & $011000101$ & $000100010$ & $101111100$ & $110111101010$ \\
    $221$ & $430$ & $197$ & $34$ & $380$ & $3,562$ \\
    PGD Offset & P4D offset & PUD Offset & PMD Offset & PTE Offset & Data Page Offset \\
  \end{tabular}
\end{center}

Each process will have a set PGD specified so this will be known by the hardware
before decoding. It therefore reads from offset 221 in this table to obtain the
physical address of the relevant P4D page table, reads from offset 430 in this
table to obtain the physical address of the relevant PUD page table, reads from
offset 197 in this table to obtain the physical address of the relevant PMD
table, reads from offset 34 in this table to obtain the physical address of the
relevant PTE table, reads from offset 380 in this table to obtain the data page
address and finally offsets 3,562 bytes into the data page to find our final
physical address.\\

In diagrammatic form, the page table structures are as below, each entry in a
page table pointing to the next before finally we determine the data page to
finally offset into:-

\input{figures/page-table-levels.tex}
